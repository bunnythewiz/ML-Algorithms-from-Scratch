# -*- coding: utf-8 -*-
"""Linear-Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b3v01Ge81YQqqxCPXJmo_4PwRSvU0Lrd
"""

# Import and mount to access drive
from google.colab import drive
drive.mount('/gdrive')

# Import required modules
import pandas as pd
import numpy as np

import warnings

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

#ignore warnings
warnings.filterwarnings('ignore')

# First convert the raw data into a dataframe as it is CSV put sep = ','
url = "https://raw.githubusercontent.com/devzohaib/Simple-Linear-Regression/master/tvmarketing.csv"
df = pd.read_csv(url, sep = ",", skiprows = 0, header = 0)

# Initial data
df

# The required scatter plot between budget and sales
plt.scatter(df['TV'], df['Sales'], c ="blue")
plt.xlabel("TV")
plt.ylabel("Sales")
plt.show()

"""* We can see an almost linear type of relation between sales and budget"""

# Now for the basic statistical measures
df.describe()

"""### Data pre-processsing"""

# Amount of missing data
print("Percentage of missing values:")
print(((df.isna().sum())/data_set.shape[0])*100)

"""* We see that there is no missing data
* Also here we see that data is not comparable so we need to normalize
"""

sales_min = np.min(df['Sales'])
sales_max = np.max(df['Sales'])

df['Sales'] = (df['Sales'] - sales_min) / (sales_max - sales_min)
df['TV'] = df['TV'] / 100
df

df.describe()

# Splitting Data
x = df["TV"]
y = df["Sales"]
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, random_state = 1)

# Checking
print(len(x_train), len(x_test))

"""* Gradient Descent"""

# Now let the fit be y = mx + c (Hypothesis Function)
# Initial values of slope and intercept be
m = 0
c = 0
# Let the learning rate be l
L = 0.01
# Let the size of test data be n and declare it
n = len(x_train)
# Define epoch (No of iterations)
epoch = 1000
# Initial MSE
print("MSE before :", (1/n)*np.sum((y_train - list(m*x_train + c))**2))
# List to store MSE
mse = []
for i in range(epoch):
  # let y_pred be declared
  y_pred = list(m*x_train + c)
  # Now for the MSE cost function to determine m and c
  MSE = (1/n)*np.sum((y_train - y_pred)**2)
  mse.append(MSE)
  # Let the derivatives of m and c be denoted as Dm and Dc
  Dm = (-2/n)*np.sum(x_train*(y_train - y_pred))
  Dc = (-2/n)*np.sum(y_train - y_pred)
  # Later we now have to update the values of m and c
  m -= L*Dm
  c -= L*Dc
print("MSE after :", (1/n)*np.sum((y_train - y_pred)**2))

#plot the cost
fig, ax = plt.subplots()
ax.plot(np.arange(1000), mse, 'r')
ax.set_xlabel('Iterations')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')

"""* We see a considerable decerese in MSE and the final MSE is okay to be taken"""

# Final m and c values
print(m, c)

# Line plotting on scatter line
x_val = np.arange(0.0, 3, 0.01)
y_val = m*x_val + c
plt.plot(x_val, y_val, 'r', linewidth=3)

plt.scatter(df['TV'], df['Sales'], c ="cyan")

# Now to compute the MSE and Absolute error
print("MSE of test data :", (1/len(y_test))*np.sum((y_test - list(m*x_test + c))**2))
print("Absolute Error of test data :", (1/len(y_test))*np.sum(abs(y_test - list(m*x_test + c))))

"""### 3"""

data = pd.read_csv('/gdrive/MyDrive/bostonHousingData.csv')

# Initial data
data

# Now for the basic statistical measures
data.describe()

cols=data.columns.tolist()
cols.remove('MEDV')
fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15, 10))

#draw the scatter plot of each input column with output column
# Flatten the 3x4 array of subplots into a 1D array
axes = axes.flatten()

# Plot each scatter plot
for i, col in enumerate(cols):
    x = data[col]
    y = data['MEDV']
    axes[i].scatter(x, y, color='b', alpha=0.7)
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('MEDV')

# Adjust layout
plt.tight_layout()

# Show theÂ plot
plt.show()

"""### Data pre-processing"""

# Amount of missing data
print("Percentage of missing values:")
print(((data.isna().sum())/data.shape[0])*100)

# Replacing the NaN values with their mean
data = data.fillna(data.mean())

# Checking
print("Percentage of missing values:")
print(((data.isna().sum())/data.shape[0])*100)

# Normalising the data
data = ((data - data.min()) / (data.max() - data.min()))

# Visualisation
data

# Splitting the data

# Creating a dataframe with 80%
# values of original dataframe
trainu = data.sample(frac = 0.8)

# Creating dataframe with
# rest of the 20% values
testu = data.drop(trainu.index)

trainu

# visualisation
testu

# Now converting them to corresponding train test datas
x_train = trainu.drop(columns = 'MEDV').to_numpy()
y_train = trainu['MEDV'].to_numpy()
x_test = testu.drop(columns = 'MEDV').to_numpy()
y_test = testu['MEDV'].to_numpy()

# Correlation Matrix
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='viridis', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()
print()

# Initialize weights with zero
# Hypothesis function Y = b0 + b1X1 + b2X2 + ...... + b13X13
beta = np.zeros(13)
# Epoch = num_iterations
epochu = 1000
# Learning rate L
L = 0.01
# Intercept
inter = 0
# Perform gradient descent
costu = []
for iteration in range(epochu):
    # Compute predictions
    y_pred = np.dot(x_train, beta)
    y_pred += inter

    # Compute errors
    errors = y_train - y_pred

    # Update coefficients
    gradient1 = np.dot(x_train.T, errors) * (-2 / len(x_train))
    beta = beta - L * gradient1
    gradient2 = (-2 / len(x_train)) * np.sum(errors)
    inter -= L * gradient2

    # Compute the new cost function
    cost = np.sum(errors**2) / (len(x_train))
    costu.append(cost)

# Resulting coefficients
print("Final Coefficients:", beta)
print("Final intercept:", inter)

# Assuming a two-dimensional dataset
x1 = np.linspace(-1, 1, 100)
x2 = np.linspace(-1, 1, 100)

# Creating a meshgrid from x1 and x2
X1, X2 = np.meshgrid(x1, x2)

# Using the provided coefficients and intercept to calculate the predicted values
Z = (-0.0347581 * X1 +
      0.09569363 * X2 -
     0.04174526 * X1**2 +
      0.097802 * X2**2 -
     0.02313165 * X1 * X2 +
      0.30210461 * np.sin(X1) +
      0.02129763 * np.cos(X2) +
      0.00695487 * np.sin(X1**2) +
      0.01694941 * np.cos(X2**2) -
     0.04718025 * np.sin(X1 * X2) -
     0.10044957 * np.cos(X1) +
      0.18364303 * np.sin(X2) -
     0.20922737 * np.cos(X1 + X2) +
      0.20049935302987465)

# Plotting the contour plot
plt.contourf(X1, X2, Z, levels=20, cmap='viridis')
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Contour Plot of the Linear Model')
plt.colorbar(label='Predicted Value')
plt.show()

#plot the cost
fig, ax = plt.subplots()
ax.plot(np.arange(1000), costu, 'r')
ax.set_xlabel('Iterations')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')

# Error on test data
y_pred = np.dot(x_test, beta)
errors = y_pred - y_test
cost = np.sum(errors**2) / (len(x_test))
print(cost)