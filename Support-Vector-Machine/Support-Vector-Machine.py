# -*- coding: utf-8 -*-
"""Linear-Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b3v01Ge81YQqqxCPXJmo_4PwRSvU0Lrd

# Modules
"""

# Import and mount to access drive
from google.colab import drive
drive.mount('/gdrive')

# Python Image Library
from PIL import Image

import numpy as np

import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import plotly.graph_objects as go

import random

# Import KMeans module
from sklearn.cluster import KMeans

"""# Support Vector Machines (SVM)

## Task-1(a)
"""

from sklearn import datasets
iris = datasets.load_iris(as_frame = True)

# Extract petal length and petal width features
X = iris.data[['petal length (cm)', 'petal width (cm)']]

# Extract target classes
y = iris.target

# Checking
print(X)
print(y)

"""As per categorical encoding for target y :
* 0 -> setosa
* 1 -> versicolor
* 2 -> virginica
"""

# Filtering to include only 'setosa' and 'versicolor' classes
X = X[y.isin([0, 1])]
y = y[y.isin([0, 1])]

plt.figure(figsize=(6, 5))

XX = X.to_numpy()
yy = y.to_numpy()

plt.scatter(XX[:, 0], XX[:, 1], c=y, cmap=plt.cm.Paired, s=20)
plt.xlabel('Petal Length (cm)')
plt.ylabel('Petal Width (cm)')
plt.title('Scatter Plot of X')

plt.tight_layout()
plt.show()

"""* Although there are 100 data points in X I don't see that many in the scatter
* Let's check for duplicates
"""

total_duplicates = X.duplicated().sum()
print("Total number of duplicate rows:", total_duplicates)

from sklearn.preprocessing import StandardScaler
import pandas as pd

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the data
X_ = scaler.fit_transform(X)

# Convert the normalized data back to a DataFrame
X = pd.DataFrame(X_, columns = X.columns)

# Checking
print(X)

from sklearn.model_selection import train_test_split

# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,
                                                    random_state = 42)

# checking
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

"""## Task-1 (b)"""

from sklearn.svm import LinearSVC

# Train LinearSVC on the training data
clf = LinearSVC()
clf.fit(X_train, y_train)

"""* The seperating Hyperplane has an equation : ax1 + bx2 + c = 0
* a, b are given coefficients and c is the intercept
"""

# Values of a and b
clf.coef_

# Value of c
clf.intercept_

# dataframe to numpy for easy operations
x1 = X_train.to_numpy()
y1 = y_train.to_numpy()

# For plotting
x = np.arange(x1.min() - 1, x1.max() + 1, 0.01)
y = -(clf.intercept_[0] + (clf.coef_[0][0] * x))/clf.coef_[0][1]

"""**Support Vectors :**
* To include the support vectors of each class I am going to use the help of
decision_function which outputs the distance od each input/training data along with sign corresponding to which class it belongs
* Then I will sort them and get the nearest points
* Now that the support vectors are parallel to hyperplane and distance between two parallel lines is mod(c1 - c1) just add the distance to original hyperplane
"""

# Get distances of each point in input data
Z = clf.decision_function(x1)
Z = sorted(Z)
# Sort and get the least from each class
sv1, sv2 = 0, 0
for i in range(len(Z)):
  if Z[i] > 0:
    sv1 = Z[i - 1]
    sv2 = Z[i]
    break

# For plotting
ys1 = -(clf.intercept_[0] + (clf.coef_[0][0] * x) + sv1)/clf.coef_[0][1]
ys2 = -(clf.intercept_[0] + (clf.coef_[0][0] * x) + sv2 )/clf.coef_[0][1]

# Train data
plt.scatter(x1[:, 0], x1[:, 1], c=y1, cmap=plt.cm.Paired, s=20)
# Hyperplane
plt.plot(x, y, linestyle='-', c = 'black')
# Support vector 1
plt.plot(x, ys1, linestyle='--', c = 'black')
# Support vector 2
plt.plot(x, ys2, linestyle='--', c = 'black')

# Add title and labels
plt.title('Decision Boundary on Train data')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')

# Show the plot
plt.show()

# Make predictions
predictions = clf.predict(X_test)

# dataframe to numpy for easy operations
x2 = X_test.to_numpy()
y2 = predictions

# Predicted data
plt.scatter(x2[:, 0], x2[:, 1], c=y2, cmap=plt.cm.Paired, s=20)
# Hyperplane
plt.plot(x, y, linestyle='-', c = 'black')
# Support vector 1
plt.plot(x, ys1, linestyle='--', c = 'black')
# Support vector 2
plt.plot(x, ys2, linestyle='--', c = 'black')

# Add title and labels
plt.title('Decision Boundary on Test data')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')

# Show the plot
plt.show()

"""## Task-2 (a)"""

from sklearn.datasets import make_moons
# Generating synthetic dataset with make_moons
X, y = make_moons(n_samples=500, noise=0.05, random_state=42)

# Introduce 5% noise by randomly flipping 5% of labels
np.random.seed(42)  # for reproducibility
num_noise_points = int(0.05 * len(y))  # 5% of the total data points

# Randomly select indices to flip labels
noise_indices = np.random.choice(np.arange(len(y)), num_noise_points, replace=False)

# Flip the labels for the selected indices
y[noise_indices] = 1 - y[noise_indices]

# Now X contains the features and y contains the labels with 5% noise

# Plot the dataset
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')
plt.title('synthetic data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

"""## Task-2 (b)"""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# Linear
svm_linear = SVC(kernel='linear')
svm_linear.fit(X, y)

plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')
plt.title('Linear Kernel')

x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
XX, YY = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = svm_linear.predict(np.c_[XX.ravel(), YY.ravel()])
Z = Z.reshape(XX.shape)
plt.contourf(XX, YY, Z, colors=['r', 'b'], levels=[0, 0.5, 1], alpha=0.3)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

y_pred = svm_linear.predict(X)

# Confusion Matrix
cm = confusion_matrix(y,y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm)
cm_display.plot()
plt.show()

# Classification Report
print(classification_report(y, y_pred))

# Polynomial
svm_poly = SVC(kernel='poly')
svm_poly.fit(X, y)

plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')
plt.title('Polynomial Kernel')

x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
XX, YY = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = svm_poly.predict(np.c_[XX.ravel(), YY.ravel()])
Z = Z.reshape(XX.shape)
plt.contourf(XX, YY, Z, colors=['r', 'b'], levels=[0, 0.5, 1], alpha=0.3)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

y_pred = svm_poly.predict(X)

# Confusion Matrix
cm = confusion_matrix(y,y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm)
cm_display.plot()
plt.show()

# Classification Report
print(classification_report(y, y_pred))

# RBF
svm_rbf = SVC(kernel='rbf', C=1, gamma=0.5)
svm_rbf.fit(X, y)

plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')
plt.title('RBF Kernel')

x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
XX, YY = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = svm_rbf.predict(np.c_[XX.ravel(), YY.ravel()])
Z = Z.reshape(XX.shape)
plt.contourf(XX, YY, Z, colors=['r', 'b'], levels=[0, 0.5, 1], alpha=0.3)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

y_pred = svm_rbf.predict(X)

# Confusion Matrix
cm = confusion_matrix(y,y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm)
cm_display.plot()
plt.show()

# Classification Report
print(classification_report(y, y_pred))

"""## Task-2 (c)"""

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {'C': [0.1, 1, 10, 100],
              'gamma': [0.1, 0.5, 1, 5, 10]}

# Create an SVM model with RBF kernel
svm_rbf = SVC(kernel='rbf')

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=svm_rbf, param_grid=param_grid, cv=5, scoring='accuracy')

# Perform grid search on the data
grid_search.fit(X, y)

# Print all accuracies
print("All accuracies:")
means = grid_search.cv_results_['mean_test_score']
stds = grid_search.cv_results_['std_test_score']
params = grid_search.cv_results_['params']
for mean, std, params in zip(means, stds, params):
    print("Mean accuracy: {:.3f}, Std: {:f}, Parameters: {}".format(mean, std, params))

# Print the best hyperparameters found
print("Best hyperparameters found:")
print(grid_search.best_params_)

"""## Task-2 (d)"""

# RBF
svm_rbf_best = SVC(kernel='rbf', C=0.1, gamma=5)
svm_rbf.fit(X, y)

plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')
plt.title('RBF Kernel')

x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
XX, YY = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = svm_rbf.predict(np.c_[XX.ravel(), YY.ravel()])
Z = Z.reshape(XX.shape)
plt.contourf(XX, YY, Z, colors=['r', 'b'], levels=[0, 0.5, 1], alpha=0.3)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

y_pred = svm_rbf.predict(X)

# Confusion Matrix
cm = confusion_matrix(y,y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm)
cm_display.plot()
plt.show()

# Classification Report
print(classification_report(y, y_pred))