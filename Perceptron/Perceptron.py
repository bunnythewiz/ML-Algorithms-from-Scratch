# -*- coding: utf-8 -*-
"""Linear-Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b3v01Ge81YQqqxCPXJmo_4PwRSvU0Lrd
"""

import random as rand
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

import warnings
# Ignore warnings
warnings.filterwarnings('ignore')

import time

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

import matplotlib.pyplot as plt
from matplotlib.image import imread

import plotly.express as px

"""### **PERCEPTRON**

**In Task 0 we are asked to Generate a synthetic 4-dimensional dataset, my approach will be**

1.   I will initialize weights and bias randomly between -1 and 1
2.   The 4D data-points will be integers from -9 to 9
3.   The classification would result in '0'(+ve) if the activation function results in < 0 and '1'(-ve) if it results in >= 0
4.   Lastly for the size of the data I will take it as say 5000
5.   Then I have to return a data.txt, I will create a txt file in collab and save it in the drive
"""

class Data_gen:

  # Constructor
  def __init__(self):
    self.weights = None
    self.bias = None
    self.dataset = None
    self.evaluation = None

  def Weights(self):

    # Let's create a list of four random float values
    weights = []
    for i in range(4):
      weights.append(rand.uniform(-1, 1))

    # Make it an attribute of a class
    self.weights = weights
    return

  def Bias(self):

    # Now for the bias
    self.bias = rand.uniform(-1, 1)
    return

  def Data_points(self):

    # Its size is 5000 x 4
    data_points = []
    for i in range(5000):
      point = []
      for j in range(4):
        point.append(rand.randint(-9, 9))
      data_points.append(point)

    # Make it an attribute
    self.dataset = data_points
    return

  def Evaluate(self):

    # Here we are going to create the activation function using the given
    # weights and bias f(x) = w0 + w1.x1 + w2.x2 + w3.x3 + w4.x4
    result = []
    for i in self.dataset:
      if (np.dot(i, self.weights) + self.bias) < 0:
        result.append(0)
      else:
        result.append(1)

    # Make it an attribute
    self.evaluation = result
    return

"""* So we now have weights, bias, dataset and evaluation"""

# Lets generate one data-set as an example
ex = Data_gen()

# Get weights
ex.Weights()
print(ex.weights)

# Get Bias
ex.Bias()
print(ex.bias)

# Get dataset
ex.Data_points()
print(rand.sample(ex.dataset, 10))

# Get classifiaction
ex.Evaluate()
print(rand.sample(ex.evaluation, 10))

# Now for easier operations lets create a df
df = pd.DataFrame(ex.dataset, columns=['X1', 'X2', 'X3', 'X4'])
df['class'] = ex.evaluation

# Visualisation
df.sample(10)

# Creating a txt file and writing
# The data_set as txt
f = open("data.txt", 'w')
print(df.shape[0], file = f)
for i, k in df.iterrows():
  print(df['X1'].iloc[i], df['X2'].iloc[i], df['X3'].iloc[i], df['X4'].iloc[i],
        df['class'].iloc[i], file = f)
f.close()

"""**We have to take a txt file as in input then, :**


*   read the txt file [Getting our train test data]
*   The target and features will be seperated as Y and X
*   As in the task 3 the split ratio is different let us ask that in the form of input
*   Split the data using train_test_split
*   Give the train data as arguments to perceptron class object method
*   Finally use the test data to evaluate accuracy for task 2
"""

# Reading the file and creating the dataset accordingly
# txt is string, convert to int via split and map
# Then seperate feature 'X' and target 'Y'
f = open("data.txt", 'r')
once  = 1
X = []
Y = []
for line in f:
  if(once == 1):
    once -= 1
    continue
  else:
    oka = list(map(float, line.split()))
    X.append(oka[:4])
    Y.append(oka[4])

# Converting into array
X = np.array(X)
Y = np.array(Y)

# Splitting data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                            train_size = 0.7, random_state = 1)

X_train

# Creating a txt file and writing
# The data_set as txt
f = open("train.txt", 'w')
print(len(X_train), file = f)
for i in list(zip(X_train, Y_train)):
  print(i[0][0], i[0][1], i[0][2], i[0][3],
        i[1], file = f)
f.close()

# Creating a txt file and writing
# The data_set as txt
f = open("test.txt", 'w')
print(len(X_test), file = f)
for i in X_test:
  print(i[0], i[1], i[2], i[3], file = f)
f.close()

# Checking
X_train.shape

"""**In Task 1 we are asked to write training code for the perceptron learning algorithm, my approach**

1.   It will have two attributes weights and bias
2.   The method activation function for classification
3.   The method fit to keep changing weights based on misclassifaction
"""

class Perceptron:

  # Constructor
  def __init__ (self):
    # Attributes for weights(w) and bias(b)
    # Initilise the weights and bias to 0
    self.w = [0, 0, 0, 0]
    self.b = 0

  # Activation_func
  def activation_func(self, x):
    return 1 if (np.dot(self.w, x) + self.b >= 0) else 0

  # We first predict the class
  # Then use it to check for misclassification
  def predict(self, X):
    Y = []
    for x in X:
      result = self.activation_func(x)
      Y.append(result)
    return np.array(Y)

  def train(self, X, Y):
    # Learning rate
    lr = 0.05

    # Get prediction data
    Y_pred = self.predict(X)

    # Running loop till convergence
    while((Y == Y_pred).all() == False):
      for x, y in zip(X, Y):
        y_pred = self.activation_func(x)
        if y == 1 and y_pred == 0:
          self.w = self.w + lr * x
          self.b = self.b + lr
        elif y == 0 and y_pred == 1:
          self.w = self.w - lr * x
          self.b = self.b - lr
      Y_pred = self.predict(X)

"""**Another observation that I made here was that the hyperparameter lr{Learning Rate} doesn't change the convergence time by much here I am not using epoch or loss function because I am sure it will converge**

*   lr = 0.01 -> --- 32.558263540267944 seconds ---
*   lr = 0.05 -> --- 32.847246170043945 seconds ---
*   lr = 00.1 ->  --- 33.67516469955444 seconds ---
*   lr = 001 ->   --- 32.027671813964844 seconds ---


"""

# Training
pp = Perceptron()
pp.train(X_train, Y_train)

"""**Task 2 We need to test model on testing data and report the accuracy**"""

# Evaluation
result = pp.predict(X_test)
print(result)

Y_test

# Accuracy
correct = 0
for i in range(len(X_test)):
  if(result[i] == Y_test[i]):
    correct += 1
print((correct / len(X_test))*100)